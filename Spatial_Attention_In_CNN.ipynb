{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNz6SbAO5NA7VBdUPxnP/27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/An-Image-Processing-Repo/blob/main/Spatial_Attention_In_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Spatial Attention in CNNs**\n",
        "\n",
        "\n",
        "\n",
        "### üîπ **What is Spatial Attention in CNN?**\n",
        "\n",
        "In CNNs, feature maps contain a lot of information, but not every **region (spatial location)** is equally important.\n",
        "\n",
        "* Some areas of the image matter more for prediction (like the object region),\n",
        "* Other areas may be background noise.\n",
        "\n",
        "**Spatial Attention** helps the model **focus on \"where\" the important features are located** in the image.\n",
        "\n",
        "\n",
        "\n",
        "### üîπ**How It Works**\n",
        "\n",
        "1. Take the feature maps from a CNN layer (say size $H \\times W \\times C$).\n",
        "\n",
        "   * $H, W$ = height & width of the feature map\n",
        "   * $C$ = number of channels\n",
        "\n",
        "2. **Compress along channels**\n",
        "\n",
        "   * Apply **max pooling** and **average pooling** along the channel axis ‚Üí this gives 2 feature maps of size $H \\times W \\times 1$.\n",
        "   * Why?\n",
        "\n",
        "     * Max pooling highlights the strongest features at each location.\n",
        "     * Average pooling gives general context.\n",
        "\n",
        "3. **Combine**\n",
        "\n",
        "   * Concatenate these 2 maps ‚Üí apply a convolution (usually $7 \\times 7$) ‚Üí then a **sigmoid**.\n",
        "   * This produces a **spatial attention map** of size $H \\times W \\times 1$.\n",
        "\n",
        "4. **Re-weight feature maps**\n",
        "\n",
        "   * Multiply this attention map with the original feature map ‚Üí the CNN will now focus more on important spatial regions.\n",
        "\n",
        "\n",
        "\n",
        "### üîπ **Formula**\n",
        "\n",
        "If $F \\in \\mathbb{R}^{H \\times W \\times C}$ is the input feature map:\n",
        "\n",
        "$$\n",
        "M_s(F) = \\sigma( f^{7 \\times 7}([AvgPool(F); MaxPool(F)]) )\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\sigma$ = sigmoid\n",
        "* $f^{7 \\times 7}$ = convolution with 7√ó7 kernel\n",
        "* $M_s(F)$ = spatial attention map\n",
        "\n",
        "Then the output is:\n",
        "\n",
        "$$\n",
        "F' = M_s(F) \\otimes F\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### üîπ **Intuition**\n",
        "\n",
        "Think of it like telling the CNN:\n",
        "üëâ \"Don‚Äôt waste time looking at the sky or background pixels‚Äî**focus on the object area** where the useful information is!\"\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ue_tn7GsSRAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Difference between Spatial Attention (like in CBAM)** and **Squeeze-and-Excitation (SE) blocks**.\n",
        "\n",
        "\n",
        "\n",
        "## üîπ **1. SE Net (Squeeze-and-Excitation Attention)**\n",
        "\n",
        "* **Focuses on channels (‚Äúwhat‚Äù features are important).**\n",
        "* Workflow:\n",
        "\n",
        "  1. **Squeeze**: Do global average pooling on the feature map ‚Üí compress spatial dimensions ‚Üí get a vector of size $1 \\times 1 \\times C$.\n",
        "  2. **Excitation**: Pass this through 2 fully connected (FC) layers + sigmoid ‚Üí outputs weights for each channel.\n",
        "  3. **Reweight**: Multiply these weights with the original feature maps channel-wise.\n",
        "\n",
        "‚ú® **Key Idea**:\n",
        "SE tells the model **which feature maps (channels)** are important (e.g., texture vs color vs shape).\n",
        "It answers **‚Äúwhat‚Äù to focus on**.\n",
        "\n",
        "\n",
        "## üîπ **2. Spatial Attention (Spatio Attention, e.g., CBAM)**\n",
        "\n",
        "* **Focuses on spatial locations (‚Äúwhere‚Äù is important).**\n",
        "* Workflow:\n",
        "\n",
        "  1. Apply **average pooling** + **max pooling** along channels ‚Üí get 2 spatial maps of size $H \\times W \\times 1$.\n",
        "  2. Concatenate them ‚Üí pass through convolution + sigmoid ‚Üí output attention map.\n",
        "  3. Multiply this with original feature map spatially.\n",
        "\n",
        "‚ú® **Key Idea**:\n",
        "Spatial attention tells the model **which regions in the image** are important (e.g., object vs background).\n",
        "It answers **‚Äúwhere‚Äù to focus**.\n",
        "\n",
        "\n",
        "\n",
        "## üîπ** Comparison Table**\n",
        "\n",
        "| Feature            | SE Block (Squeeze-Excitation)  | Spatial Attention (CBAM style) |\n",
        "| ------------------ | ------------------------------ | ------------------------------ |\n",
        "| **Focus**          | Channel-wise (what features)   | Spatial (where in the image)   |\n",
        "| **Mechanism**      | Global avg pooling + FC layers | Pooling across channels + Conv |\n",
        "| **Output size**    | $1 \\times 1 \\times C$          | $H \\times W \\times 1$          |\n",
        "| **Attention type** | ‚ÄúWhat is important?‚Äù           | ‚ÄúWhere is important?‚Äù          |\n",
        "| **Granularity**    | Global per-channel weights     | Local pixel/region weights     |\n",
        "\n",
        "\n",
        "\n",
        "## üîπ **Together**\n",
        "\n",
        "* **SE block = channel attention only.**\n",
        "* **Spatial attention = location attention only.**\n",
        "* **CBAM = both (channel + spatial).**\n",
        "  That‚Äôs why CBAM is often described as a **generalization of SE**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qFs7a2Q2SZ7t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_07kBnjnSGPw"
      },
      "outputs": [],
      "source": []
    }
  ]
}